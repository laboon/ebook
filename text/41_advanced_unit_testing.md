# Advanced Unit Testing

Although you can go relatively far with the skills learned in the last chapter, there are still aspects of unit testing which won't be possible with those techniques.  In this chapter, we'll go further in depth on how to construct more complex test cases.

## Test Doubles

A unit test should be a localized test; that is, it should check the particular method or function under test, and not worry about other aspects of the system.  If there is a test failure, we want to make sure that the failure is due to the code in that particular method, not something else which that method relies upon.  Software is often interconnected, and a particular method which relies upon other methods or classes may not work correctly if those other units of code do not work correctly.

In the following method, we will have fun at a duck pond.  Calling `.haveFunAtDuckPond()` with a `Duck d` will feed the duck `numFeedings` number of times.  The method will then return the amount of fun, which is directly in proportion to how many times the duck is fed.  The duck will quack each time that it is fed.  Note that we are feeding the duck pieces of regulation duck chow.  Don't feed ducks bread, it's not actually good for them!  If a null duck is passed in, or the number of feedings is zero or fewer, then it simply returns 0 as the amount of fun (null ducks and negative feeding are both equally not fun).   Let us further assume that the implementation of `Duck` is faulty, and calling the `.quack()` method results in a `QuackingException`:

```java
public int haveFunAtDuckPond(Duck d, int numFeedings) {
    if (d == null || numFeedings <= 0) { return 0; }
    int amountOfFun = 0;
    for (int j=0; j < numFeedings; j++) {
        amountOfFun++;
        d.feed();
        d.quack();
    }
    return amountOfFun;
}
```

Even though the code in this method works perfectly for all inputs, it requires that a working duck be present.  Otherwise, any non-negative number of feedings with a valid duck will cause an exception to be thrown.  If we see a unit test for this particular method failing, we will naturally think that the problem is in this method.  Only after examining it will we understand that the problem actually lies elsewhere.  How can we test this code when the code it depends upon doesn't work?

I wouldn't have asked the question if I didn't have an answer---__test doubles__.  Test doubles are "fake" objects which you can use in your tests to "stand in" for other objects in the codebase.  This has numerous benefits aside from hiding pieces of the codebase that don't work.  Test doubles also allow you to localize the source of errors.  If our tests for `haveFunAtDuckPond()` fail, then the problem should lie in that particular method, not in one of the classes or methods that the method depends upon.

JUnit does not support test doubles directly, but there are libraries that you can install alongside JUnit that do.  For the next few sections, we will use a library called  Mockito to enable doubles, mocks, verification, and stubbing.  I know we haven't defined these terms yet, but isn't it exciting to know what's coming next?

In order to use Mockito, you will need to ensure that you have both the mockito and objenesis jars in your classpath, along with any required JUnit jars.  If you use the Makefile from the previous chapter, and have downloaded the appropriate jar files, this should work for you automatically.  If you are using an IDE or a build tool (such as Gradle or Maven), this may also already have been taken care of.  If you are not using the test runner from the last chapter, you should review the documentation for your build tool to ensure that you have set it up correctly.

You should then add the line `import static org.mockito.*` to your test class files in order to access all of the tools in the libraries.  As a reminder, using `import static` instead of `import` will allow us to access the static members of the mockito classes without explicitly referring to them.

Here is an example of using a test double with JUnit and Mockito to test a method which relies on a test double object.  Note that Mockito calls all test doubles "mocks", even if they don't use the capabilities of a mock object (described later in the chapter):

```java
// Class to test
public class Horse {

    public int leadTo(Water w) {
        w.drink();
        return 1;
    }

}
```

```java
// Unit test for class
import static org.junit.Assert.*;
import static org.mockito.*;
import org.junit.*;

public class HorseTest {

    // Test that leading a horse to water will return 1
    @Test
    public void testWaterDrinkReturnVal() {
        Horse horse = new Horse();
        // We are making a test double for water
        Water mockWater = mock(Water.class);
        int returnVal = horse.leadTo(mockWater);
        assertEquals(1, returnVal);
    }

}
```

We have now created a "fake" object instead of passing in an actual instantiation of the Water class.  Water is now "quarantined" and cannot cause a failure in our code.  If the test fails, it's the fault of this particular method.  Therefore, whenever there is a failure, we'll know exactly where to look in the codebase to determine the issue.  Whenever you use doubles, however, you are also dependent upon your assumptions of how the code you depend upon is supposed to work in the actual program.  There is nothing stopping you from, say, creating test doubles that have a method that the actual class does not.  In this case, your tests will work fine but the actual program will not.

Doubles can also be used to speed up the execution of tests.  Think of a Database object which writes information out to a database and returns the status code.  Under ordinary circumstances, the program needs to write out to disk and perhaps even access the network.  Let's say that this takes one second.  This may not seem like an incredibly long time to wait, but multiply it by all the tests that access the database.  Even a relatively small program may have hundreds or even thousands of unit tests, adding minutes or hours to the amount of time each test run takes.

You should never, ever, ever use a double for the current class under test!  If you do this, you're no longer testing anything, really, as you are creating a fake object for the actual thing that you are testing.

Test doubles should be used, as often as possible, when the code that you are unit testing uses a different class than the class under test.  Sometimes it will be difficult to do.  In order to minimize issues, you should pass in the object as a parameter whenever possible, as opposed to relying on member variables or global variables.  Even worse, and more common, are methods that generate and use objects entirely internally.  It is often not possible to use test doubles for these methods.  Let's take a look at an example of this:

```java
public class Dog {

    DogFood _df  = null;
    DogDish _dd  = null;
    DogWater _dw = null;

    public void setUpDogStuff() {
        _dd = new DogDish();
        _df = new DogFood();
        _dw = new DogWater();
    }

    public int eatDinner() {
        _df.eat();
        return 1;
    }

}
```

If we were to write a test for this class, we have no way of making doubles for the objects internal to the class!  Even if we then refactored `setUpDogStuff()` to accept `DogDish`, `DogFood`, and `DogWater` parameters, we would be forced to work with additional items when all we care about is `DogFood`.

Let's refactor the method a bit to make it somewhat more amenable to test doubles:

```java
public class Dog {

    DogFood _df = null;
    DogDish _dd  = null;
    DogWater _dw = null;

    public void setDogFood(DogFood df) {
        _df = df;
    }

    public void setDogDish(DogDish dd) {
        _dd = dd;
    }

    public void setDogWater(DogWater dw) {
        _dw = dw;
    }

    public int eatDinner() {
        _df.eat();
        return 1;
    }

}
```

If we wanted to test this, we would have to create an object and set the value separately from the actual test execution:

```java
public class DogTest {

    @Test
    public void eatDinnerTest() {
        Dog d = new Dog();
        d.setDogFood(mock(DogFood.class));
        int returnVal = d.eatDinner();
        assertEquals(1, returnVal);
    }

}
```

This is better, but still not ideal, as extra statements are still required to set up the test.  If, instead, we just pass in `DogFood` as a parameter to the method, like so:

```java
public class Dog {

    public int eatDinner(DogFood df) {
        df.eat();
        return 1;
    }

}
```

The test would then look like:

```java
public class DogTest {

    @Test
    public void testEatDinner() {
        Dog d = new Dog();
        int returnVal = d.eatDinner(mock(DogFood.class));
        assertEquals(1, returnVal);
    }

}
```

This will enable much easier and much more focused testing.  You'll also note the code has several other benefits aside from increased testability.  It's easier to read and understand, it's shorter, and there are fewer chances for errors.  In the original version, it would be very easy for a programmer to forget to set the `_df` variable to a `DogFood` object at some point, causing a null pointer exception whenever the dog tried to eat.  While still possible, this is less likely when you are passing in the object directly to the method.  We'll discuss more of the benefits of writing testable code---and why testable code is good code, and vice versa---in a later chapter.

## Stubs

If doubles are fake objects, stubs are fake methods.  In the above examples, we didn't care what calling `.eat()` on the `DogFood` object did; we just didn't want it to call an actual `DogFood` object.  In many cases, though, we expect a certain return value when a method is called.  Let's modify the `.eat()` method on `DogFood` so that it returns an integer indicating how tasty the dog food is:

```java
public class Dog {

    public int eatDinner(DogFood df) {
        int tastiness = df.eat();
        return tastiness;
    }

}
```

If we were just using `df` as a normal test double, then there is no telling what `df.eat()` will return.  Specifically, the answer varies by test framework---some will always return a default value, some will call out the real object, some will throw an exception.  This should just be a piece of trivia, though---you shouldn't call methods on a double object unless you have stubbed them.  The whole reason for making a test double is so that you have an object that you have specified, instead of relying on external definitions.  So let's say that our doubled `DogFood` object has a (scientifically determined) tastiness level of 13.  We can specify that whenever the `.eat()` method is called on our doubled object, then just return a value of 13.  We have made a __stub__ of the method:

```java
public class DogTest {

    @Test
    public void testEatDinner() {
        Dog d = new Dog();
        DogFood mockedDogFood = mock(DogFood.class);
        when(mockedDogFood.eat()).thenReturn(13);
        int returnVal = d.eatDinner(mockedDogFood);
        assertEquals(13, returnVal);
    }

}
```

Now when the `mockedDogFood` object has its `.eat()` method called, it will return the value 13.  Once again, we have quarantined the other methods by re-creating fakes of them that act as we think they should.  In some languages, although not Java, we can even stub methods that don't exist.  This allows us to not only test classes that have errors, but even ones that don't have all of their methods written yet.

## Mocks and Verification

"Yes, yes, this is all fine," I can hear you saying, "but you didn't answer the original question!  We are still dependent on asserting on a value that is returned from a method, and thus won't be able to test methods without a return value!"  Remember the impure method `goToCatCafe` that we wanted to test in the last chapter:

```java
public class World {

    public void goToCatCafe(CatCafe catCafe) {
        System.out.println("Petting cats at a Cat Café!");
        catCafe.arrive();
        catCafe.haveFun();
    }

}
```

There is no return value and thus nothing on which to assert.  The only way to test this method is to ensure that the `.arrive()` and `.haveFun()` methods on the `catCafe` object were called.  We can do this using a special kind of test double called a __mock__.  A mock object will allow you to assert that a particular method was called on the mocked object.  In the above instance, instead of asserting that a particular value is returned (since no value is ever returned), you instead can make "meta-assertions" that `.haveFun()` and `.arrive()` are called.  This is called __verification__ since you are _verifying_ that a method has been called.  Note that this kind of verification has nothing to do with the kind of verification that means "checking that the software is right".  They are two different concepts that happen to share the same word.

Here is how we might test that method using verification.

```java
@Test
public void testGoToCatCafe() {
    CatCafe cc = mock(CatCafe.class);
    World w = new World();
    w.goToCatCafe(cc);
    verify(cc.arrive());
    verify(cc.haveFun());

}
```

Note, in this case, that there is no traditional assertion (e.g., `assertEquals`).  The assertions are "hidden" inside the `verify()` call.  If verification fails, the test case will fail just as it would with a simple assertion.

You may want to check that is called a certain number of times.  Let's assume that we have a method `.multiPet()` which will allow you to easily pet the same cat numerous times, based on a passed-in integer.

```java
public void multiPet(Cat c, int numTimes) {
    for (int j = 0; j < numTimes; j++) {
        c.pet();
    }
}
```

Using the `times()` method along with verification, you can check that if you call `multiPet(c, 5)`, that the cat C will be petted 5 times, or if you call it with the numTimes argument equal to 900 that cat will be petted 900 times (lucky cat!).

```java

/**
 * Check that if we call multiPet with a valid cat and a numTimes
 * argument of 5, that cat will be petted 5 times.
 */

@Test
public void testMultiPet5() {
    World w = new World();
    Cat c = mock(Cat.class);
    w.multiPet(c, 5);
    verify(c.pet(), times(5));
}
```

Conversely, you may also want to check that a method is never called.  Let's imagine another CatCafe-related method in our `World` class, `.petCat()`.

```java

public void petCat(Cat c, boolean gentle) {
    if (gentle) {
        c.pet();
    } else {
        // Do nothing - don't pet cats without being gentle!
    }
}
```

In this case, we want to test that if the `gentle` variable is true, `.petCat()` will be called, otherwise it will not be called.  We can create two unit tests for each of these equivalence classes.

```java
/**
 * Check that attempting to pet a cat while being gentle will
 * cause the cat to be petted one time.  This is done by verifying
 * that c.pet() is called one time.
 */

@Test
public void testPetCatGently() {
    World w = new World();
    Cat c = mock(Cat.class);
    w.petCat(c, true);
    verify(c.pet(), times(1));
}

/**
 * Check that attempting to pet a cat without being gentle will
 * not cause the cat to be petted.  This is done by verifying
 * that c.pet() is never called.
 */

@Test
public void testPetCatNotGently() {
    World w = new World();
    Cat c = mock(Cat.class);
    w.petCat(c, false);
    verify(c.pet(), never());
}
```

We could also replace the `never()` method call with `times(0)`, as they are equivalent.

In summary, verify is used to make an assertion on the executed code.  That is, unlike a traditional assertion which checks that a value is correct, a `verify` call asserts that a method was called (or not).

Note that we did not test whether or not the `System.out.println()` call worked.  We will see how to do this later in this chapter, in the section _Testing System Output_.

## Fakes

Sometimes, you need to have a test which depends on an object, and will require complex or non-performant behavior.  In this case, you can use a __fake__.  A fake is a special kind of test double which acts similarly to the regular object.  However, it is written to be a part of the test, meaning that it runs faster and simpler.  For example, you may remove any parts of the code which write to disk (always a slowdown when writing tests).  You may have it perform a simpler calculation rather than a very time-consuming one.  You may reduce that object's dependencies on other objects.

Fakes require more work to create than a simple test double, since they are a "lite" version of an object instead of simply specifying its external behavior.  However, this allows you to perform more intricate tests than the relatively simple ones possible with test doubles.  Suppose your DuckPond class is as follows:

```java
public class DuckPond extends Pond {

   private int _funLevel = 0;

   public void haveFun() {
      _funLevel++;
   }

   public void haveUltraFun() {
      int funMultiplier = super.retrieveUltraLevelFromDatabase();
      _funLevel += funMultiplier * _funLevel;
   }

   public int getFunLevel() {
      return _funLevel;
   }

}
```

Let's set aside any issues one might have about how this code is written, and how it might be refactored to make it more testable.  In this case, calling `haveUltraFun()` requires querying the database, which is called from a method from DuckPond's superclass.  However, this also modifies the `_funLevel` variable, based on the value it received from the `retrieveUltraLevelFromDatabase()` call.  The value of the variable `_funLevel` is going to depend both on that call to the database as well as how many times `haveFun()` and `haveUltraFun()` have been called.  While it would be possible to just make a test double that returns specific values, adding in this behavior for a test which called multiple methods on a DuckPond object might add up to lots of extra work.  Even worse, this work might have to be replicated in multiple tests.

Using DuckPond as-is also means that every call to `haveUltraFun()` will result in a dramatic slowdown in tests.  Remember that calls to the disk or network, since they make take an order of magnitude or longer in time than the test would take otherwise, are discouraged in unit testing.

To get around this performance issue, let's create a fake version of the object, which we can use in our tests later.  This fake version will be a "stripped-down" version of DuckPond, but will keep the general behavior.

```java
public class FakeDuckPond extends Pond {

   private int _funLevel = 0;

   public void haveFun() {
      _funLevel++;
   }

   public void haveUltraFun() {
      // REMOVED DATABASE CALL
      _funLevel += 5 * _funLevel;
   }

   public int getFunLevel() {
      return _funLevel;
   }

}
```

We are now assuming that the `funMultiplier` variable retrieved from the database will always be 5.  This will both speed up tests (by removing the database read) and simplify calculations (as well as help us understand what the expected behavior should be).  However, unlike with a traditional test double or mock, we don't have to specify what the behavior should be externally.  The class itself will determine the (simplified) behavior.

## Setup and Teardown

Oftentimes, there are particular things that you want the tests for a particular class to do before each of the tests start, and after each of the tests end.  For example, you may set up a mocked database connection, and want to use it as a precondition for all of the tests, instead of repeating the same line of code to create it in each individual test case.  JUnit provides annotations for creating `setUp()` and `tearDown()` methods for cases such as this.  Specifically, you can add an `@Before` annotation for any methods you want to have run before each individual test case, and an `@After` annotation for any methods you want to run after.  Although you could theoretically name these methods something other than `setUp()` and `tearDown()`, these are common terms that you will see very often.

Here is an example of using `setUp()` and `tearDown()` methods:

```java
public class BirdTest {

    DatabaseConnection _dbc = null;

    // Set up a mocked database connection
    @Before
    public void setUp() throws Exception {
        mockedDbc = mock(DatabaseConnection.class);
        _dbc = setupFakeDbConnection(mockedDbc);
    }

    // Tear down our mocked database connection
    @After
    public void tearDown() throws Exception {
        _dbc = null;
    }

    // Tests that a newly created bird has a default fluffiness
    // level of 1
    @Test
    public void testFluffyBird() {
        Bird b = new Bird(_dbc);
        assertEquals(1, b.fluffinessLevel);
    }

    // Tests that a newly created bird is pretty
    @Test
    public void testPrettyBird() {
        Bird b = new Bird(_dbc);
        assertTrue(b.isPretty());
    }

}
```

Note that `@Before` and `@After` methods are called before _each_ test case, not one time before all of them and one time after all of them.  In the above instance, `setUp` will be called twice and `tearDown` will be called twice.

Also note that while there is nothing stopping you from prefacing multiple methods with `@Before` or `@After` annotations, it's usually not necessary and will just make it more difficult to follow the code.  If you do make multiple methods with these annotations, they will run in a deterministic order (that is, if you run it again, they will run in the same order).  However, since tests should not depend upon each other, this ordering is not specified and is liable to change whenever you update your code and/or JUnit version.  Thus, it's generally a good idea to just have one `@Before` and one `@After`, maximum, per test file.

Getting back to setup and teardown procedures specifically, if you have complicated ones, using multiple annotations is usually unnecessary. You can still use a single method, but call out to other methods as helpers, instead of annotating numerous methods.

## Testing System Output

One particular use case that `@After` and `@Before` annotations can help you with is checking for system output.  Console output is a very common item to check for, but testing for it in Java is non-intuitive.  Although it's possible to just pass in a System object with each method, which you could then mock and stub, this is not idiomatic Java code and will add lots of additional code (and most likely complexity) to your codebase.  The best solution that I have seen for checking it involves using the `setOut()` method of `System` to put the output of `System.out` and `System.err` in `ByteArrayOutputStream`.[^disclosure]

[^disclosure]: Full disclosure: I saw this on Stack Overflow at [http://stackoverflow.com/q/1119385](http://stackoverflow.com/q/1119385).  (Yes, even authors of books go online to look for answers sometimes.)

Here is an example of how to check for specific output from a Java program using this technique:

```java
public class Kangaroo {

    public void hop() {
        System.out.println("Hoppity hop!");
    }

}

public class KangarooTest {

    private ByteArrayOutputStream out = new ByteArrayOutputStream();

    @Before
    public void setUp() {
        System.setOut(new PrintStream(out));
    }

    @After
    public void tearDown() {
        System.setOut(null);
    }

    @Test
    public void testHop() {
        Kangaroo k = new Kangaroo();
        k.hop();
        assertEquals("Hoppity hop!", out.toString());
    }

}
```

## Testing Private Methods

There's quite an argument on whether or not it makes sense to test private methods.  It's a great way to start a flame war amongst software developers and testers on your favorite social networking site.  I'll provide you with the arguments of both sides, so that you can make a decision yourself, and then I'll let you know my opinion.

Those who argue that private methods should _never_ be tested say that any calls from the rest of the program (i.e., the rest of the world from the class's standpoint) will have to come in through the public methods of the class.  Those public methods will themselves access private methods; if they don't, then what's the point of having those private methods?  By testing only the public interfaces of classes, you're minimizing the number of tests that you have and focusing on the tests and code that matter.

Another reason for not testing private methods is that it inhibits you from refactoring the code later.  If you have already written tests for private methods, it is going to be more work to make changes to anything "behind the scenes".  In a sense, testing private methods means going against one of the key tenets of object-oriented programming, namely data hiding.  The system is going to be less flexible and more difficult to modify going forward.

Those who argue that private methods should _always_ be tested point to the fact that private methods are still code, even if they're not called directly from outside the class.  Unit testing is supposed to test functionality at the lowest levels possible, which is usually the method or function call.  If you're going to start testing higher up the abstraction ladder, then why not just do systems-level testing?

My opinion is that, like most engineering questions, the correct answer depends on what you're trying to do and what the current codebase is like.  As a side note, with most technical questions, saying "it depends" is a great way to be right, no matter what.  Let's take a look at a few examples.

Imagine that we're adding a service to Rent-A-Cat to automate taking pictures of cats:

```java
public class Picture {

    private void setupCamera() {
        // ...
    }

    private void turnOffCamera() {
        // ...
    }

    private Image captureImage() {
        // ...
    }

    public Image takePicture() {
        setupCamera();
        Image i = captureImage();
        turnOffCamera();
        return i;
    }

}
```

This code is relatively simple, and it's easy to see that all of the private methods will be called and tested by the public methods.  From the standpoint of a user of this class, it's easy to test that things work correctly---if a valid image is returned, then the method worked.

Now let's imagine in a different part of this service, we have an image transformation library that we also want to test:

```java
public class ImageLibrary {

    public Image transform(Image image, int inSize, int outSize, String format,
      boolean color, boolean reduce, boolean dither) {
        if (inSize < outSize && format.equals("jpg") || dither == false) {
            return privateMethod1(image, inSize, outSize);
        } else if (inSize < outSize && format.equals("png") || dither == true) {
            return privateMethod2(image, inSize, outSize);
        } else if (outSize > inSize || (color == false && reduce == false) {
            return privateMethod3(image, inSize, outSize);
        } else {
            // Imagine lots more if...then...else if statements here
            // You get the idea
        }
    }

    // Lots of private methods here

}
```

Think of all the complicated tests that would be needed for this one method!  Even then, you're not focusing on the actual image transformations.  Many of your tests would just be focused on ensuring that the right method was called!

One could argue that this isn't a well-designed piece of code and should be refactored, ideally with the private methods put into, say, an `ImageTransformer` class, where the methods would be public and could easily be unit tested.  I wouldn't disagree.  However, the fact of the matter is that in the real world, there is often code like this lying around, and the tester is not always in a position to tell management that the company needs to spend a few months burning off technical debt instead of adding new features.  If your goal is to test the software, and test it well, you'll probably have to test the occasional private method.  For specific information on how to do this in Java, please see the supplemental chapter on _Using Reflection to Test Private Methods in Java_ at the end of this book.

## Unit Test Structure

### Basic Outline

Unit tests in Java are usually grouped initially by class and further by method; they mirror the structure of the program.  Since unit tests are white-box tests which interact closely with the code, this makes finding errors in the codebase based upon the particular failing test case much easier than with integration or manual tests.

### What to Test?

Exactly what you test will vary based upon the domain of software you are testing and the amount of time you have for testing, as well as organizational standards and other external factors.  Of course, stating this doesn't give you any direction at all, and similar caveats could probably be put in front of every paragraph of this book.  There are some heuristics to follow, many of which directly mirror some of the items discussed when developing a test plan.

Ideally, you should look at the method and think of the various success and failure cases, determine the equivalence classes, and think about what some good boundary and interior values might be to test from those equivalence classes.  You want to also focus on testing common use cases over use cases which rarely occur, at least at first.  If you are writing more safety-critical software, often it makes sense to focus on testing for failure before checking the happy path.  Personally, I often work on a base case first, and then think of possible failure cases after that.  Oftentimes, I will go back, sometimes with a profiler, and see what code is executed most often and add extra test cases for that.  I may try to construct a mental model of what is called often instead of using a profiler.  I will definitely think of from where the inputs to the method are coming.  If they are from a system that I have no control over (including users, the ultimate example of systems I have no control over) and may be sending unanticipated values, I will definitely spend more time thinking of possible failure cases and checking for edge cases.

You don't want to create a test suite that takes so long to run that people don't run it often, but a well-designed unit test suite with appropriate doubles, mocks, stubs, and the like should run very fast even when there are many tests.  I would err on the side of creating too many tests rather than too few, at first.  As you determine how many tests are necessary for the particular piece of software you're working on, you can start making trade-offs between the amount of time for development and for testing.

### Assert Less, Name Directly

When unit tests fail, they should point you to exactly what went wrong, and where.  They should not be large "one size fits all" tests.  It is so fast to run a properly-written unit test (rarely taking more than a few tens of milliseconds) that the extra execution time and work involved in writing more tests is absolutely dwarfed by the amount of time that you will save when unit tests tell you exactly what went wrong when they failed.  A unit test with lots of assertions shows a lack of thought towards what exactly the unit test was supposed to check for.  Consider the following example:

```java
public class CatTest {

    @Test
    public void testCatStuff() {
        Cat c = new Cat();
        c.setDefaults();
        assertTrue(c.isAGoodKitty());
        assertEquals(0, c.numKittens());
        assertFalse(c.isUgly());
        assertNull(c.owner());
    }

}
```

When this test fails, what is wrong?  It could be that a newly created cat is not a good kitty, as it should be.  It could be that there's an error with the number of kittens that a newly created cat has.  It could be that the cat was erroneously assigned an owner.  You'd have to track down the particular assertion in the test code, because seeing that "cat stuff" has failed does not tell you anything.  If the name of the test case failing does not let you know what the failure is and preferably give you a very good pointer to where the failure lies, you are probably testing too much in each individual test case.  There are numerous cases where I've gone in and fixed code just based on seeing the name of the test case that failed, without ever looking at the actual test code.  This is a common consequence of well-specified, directed tests.

### Unit Tests Should Be Independent

Unit tests should not be dependent on run order.  That is, Test 2 should not depend on any side effects or result from Test 1.  JUnit, and most other unit testing frameworks, will not run individual test cases in a predetermined order.  By avoiding dependencies on each other and allowing each test to run independently, failures are localized to a particular test.   Now imagine writing the following code:

```java
public class Cat {

    private int _length;

    public Cat(int length) {
        _length = length;
    }

    public int getWhiskersLength() {
        return _length;
    }

    public int growWhiskers() {
        _length++;
        return _length;
    }
}


public class CatTest {

    int _whiskersLength = 5;

    @Test
    public void testWhiskersLength() {
        Cat c = new Cat(5);
        assertEquals(_whiskersLength, c.getWhiskersLength());
    }

    @Test
    public void testGrowWhiskers() {
        Cat c = new Cat(5);
        _whiskersLength = c.growWhiskers();
        assertEquals(6, _whiskersLength);
    }

}
```

If we run these tests with JUnit, which executes test cases in a random order, sometimes the second test would pass, and sometimes it would fail!  Why?

Let's assume that the tests are run in the order that they are listed - that is, with `testWhiskersLength()` executed first.  The variable `_whiskersLength`, which is set to its default value of 5, will be equal to the starting whiskers length in the constructed Cat object.  Thus, our assertion that the length of the whiskers is 5 will be correct.  When `testGrowWhiskers()` is called, the whiskers are grown by one unit, and the return value (i.e., the new length of the whiskers) is placed into the `_whiskersLength` variable, changing it to 6.  Since `_whiskersLength` is equal to 6, the assertion is also correct.  Congratulations, all of the tests pass!

Now let's see what happens when the tests are run in the opposite order, with `testGrowWhiskers()` executed first, then `testWhiskersLength()`.  At the end of the `testGrowWhiskers()` test, `_whiskersLength` (which is a class level variable, and thus shared by all methods) is equal to 6.  Now `testWhiskersLength()` is executed, but the new Cat object's whiskers length is 5, which is not equal to `_whiskersLength`, which is 6.  Now we have a failing test - but one that only intermittently fails, depending in which order the test are executed.

We can fix this by ensuring that the tests are not dependent on each other.  The easiest and best way to do this is to eliminate any shared data between tests.  In this case, it means not depending on the class-level variable `_whiskersLength`.  Let's rewrite the tests so that they can be run independently.

```java
public class CatTest {

    @Test
    public void testWhiskersLength() {
        Cat c = new Cat(5);
        assertEquals(5, c.getWhiskersLength());
    }

    @Test
    public void testGrowWhiskers() {
        Cat c = new Cat(5);
        int whiskersLength = c.growWhiskers();
        assertEquals(6, whiskersLength);
    }

}
```

While this example has a relatively simple solution, there may be cases where the dependencies are more difficult to determine or fix.  Whenever you find that a test only passes sometimes, you may want to first investigate if there are any "hidden" dependencies betweent tests.  These hidden dependencies may be objects or variables shared between methods, static variables on a class, or other external data on which your tests rely.

Note that in JUnit, it _is_ possible to specify the order that tests will run in, by using the `@FixMethodOrder` annotation.  However, you want to avoid doing that if at all possible.  It is all too easy to allow your tests to fall into the trap of depending on each other by ensuring that they always run in the same order.

There is yet another benefit to creating tests which have no dependencies on other tests.  Independent tests can be run in parallel, on different cores of a CPU or even on different machines entirely.  If test execution depends on running sequentially, then you will not be able to run them in parallel.  On a modern multi-core machine, you may find yourself running tests many times more quickly if they can be run independently.  While the precise speed-up will depend on the particular tests you are running, what your computer hardware is like, and other factors, you can easily see test execution times reduced by 50% or more.

### Try to Make Tests Better Every Time You Touch Them

Oftentimes, you will need to work with legacy code, including code which was poorly written or does not have good test coverage.  Although it may be tempting to simply continue working in the same manner that the code was originally written, you should try to do what you can to make the testing of the code better.  Keep your code easily testable, and if necessary, build wrappers around code that is difficult to test.

There is more discussion on writing testable code in the chapter _Writing Testable Code_ (with a title like that, what else did you expect to find in it?)

## Code Coverage

Code coverage tells you how much of the codebase is actually executed when running the test suite code.  Since defining exactly what is meant by "how much of the codebase" can be complex, there are numerous kinds of code coverage.  The simplest form of code coverage is method coverage; this measures what percentage of methods have any tests that call into them.  For example, imagine a class `Turtle` with two methods, `crawl()` and `eat()`:

```java
public class Turtle {

    public void crawl() { ... }

    public void eat() { ... }

}
```

If we have a test calling `crawl()`, but none calling `eat()`, then we have 50% code coverage (one of two methods was tested).  Even if we had a hundred tests calling `crawl()`, checking out all of the various different ways that a turtle can crawl, but none testing `eat()`, we still have 50% code coverage.  Only once we add at least one test checking `eat()` will we have 100% code coverage.

A more detailed form of code coverage is statement coverage.  This measures what percentage of programming statements were actually executed by at least one test.  A programming statement is the smallest "chunk" of code that still makes sense as an individual unit of code; in Java, these are generally the pieces of code separated by semicolons.  Some examples of statements include:

1. `int j = 7;`
2. `System.out.println("Bark!");`
3. `foo--;`

Note that "statement" does not mean "a line of code"!  For example, in Java, you may have a line such as:

```java
doSomething(k); k++;
```

This is actually multiple statements, they just happen to be on the same line.

When developers discuss "code coverage", they usually mean specifically "statement coverage".  Compared to method coverage, using statement coverage provides vastly more granularity in knowing what parts of the codebase have actually been tested.  For example, let's add some detail to our `Turtle` class so that we can see what the code is like inside the various methods:

```java
public class Turtle {

    CurrentLocation _loc = World.getCurrentLocation();
    GroundType _g = World.getGroundType(_loc);

    public void crawl() {
        if (_g == DIRT) {
            move(SLOWLY);
        } else if (_g == GRASS) {
            eat();
            move(MORE_SLOWLY);
        } else {
            move(EVEN_MORE_SLOWLY);
        }
    }

    public void eat() {
        System.out.println("Yum yum");
    }

}
```

Using method coverage, having just one test for `eat()` or just one for `crawl()` would show 50% code coverage.  This hides the fact that `crawl()` is much more complicated than `eat()` and any single test would miss several different possible outcomes, whereas `eat()` can be tested very well with just one test.  It also would not let us know what particular lines were not tested---we would have to examine the test code to determine whether it was testing that the turtle could move on dirt, grass, or whatever else.  Statement coverage output can tell us exactly which lines were never executed during a test run, so we know exactly what kinds of tests to add to ensure that we are checking that each line has been tested at least once.

There are other variants of code coverage such as branch coverage, which measures which percentage of conditionals (if statements, case statements, etc.) have been tested.  However, these other kinds of code coverage are usually only used for much more specialized testing.  Chances are that you will deal with the statement and method coverage much more often.

Having a statement or method covered by tests does not mean that all defects in that unit of code have been found by the test!  It's easy to imagine defects slipping through method coverage.  In our `Turtle` example, if there was some sort of problem when the turtle was on grass, but our test only checked the case that the turtle was on dirt, we can see how method coverage would say that `crawl()` has been checked but defects can still creep through.  At a lower level of abstraction, statement coverage does not check for all variations in the way a particular statement could be executed.  Let's consider the following single-method class and its associated test case:

```java
public class Cow {

    public int moo(int mooLevel) {
        int timesToMoo = Math.ceil(100 / mooLevel);
        for (int j=0; j < timesToMoo; j++) {
            System.out.println("moo!");
        }
        return timesToMoo;
    }

}

public class CowTest {

    @Test
    public void mooTest() {
        Cow c = new Cow();
        int mooTimes = c.moo(20);
        assertEquals(5, mooTimes);
    }

}
```

From a code coverage perspective, we have 100% code coverage and 100% method coverage---the only method in the class is called from a test, and every statement in the method is executed.  However, calling `moo()` with a `mooLevel` parameter of 0 will cause a `DivideByZeroException` to be thrown.  This defect will not be discovered by the test case, despite every statement being executed.  Although not foolproof, ensuring that you've checked every equivalence class will help to ameliorate situations like this.

In fact, code coverage metrics can be even more misleading than this example.  As long as a statement is _executed_ by the test, that code is considered "covered".  Nothing verifies that the unit tests actually test anything, though.  Consider this case:

```java
public class CowTest {

    @Test
    public void mooTest() {
        Cow c = new Cow();
        int mooTimes = c.moo(1);
        assertTrue(true);
    }

}
```

This test results in 100% code coverage of the method, but tells you almost nothing about the code.  The only information you can get from this test passing is that calling `c.moo(1)` does not crash the program.

Code coverage is a powerful tool, but like anything else in software development, it is not a silver bullet.  It's an excellent way to see what areas of the codebase might need more testing, but it should not assure you that any code that has been covered is guaranteed free of defects.  It should not even assure you that a particular piece of code has truly been tested.

Just like your favorite nutritionally-suspect cereals, unit testing should not be your complete breakfast nor your entire test plan.  Unit testing is great for checking individual methods and low-level functionality, but it is not good at seeing how everything fits together.  It's even worse when trying to determine what the end product will look like; all of the individual methods may work, but together they form something which doesn't meet any of the requirements.

When you are testing, you should remember to perform some manual testing, integration testing, and depending upon your needs, other kinds of testing such as security testing or performance testing.  Relying heavily on one particular kind of testing is a recipe for missing important defects.
