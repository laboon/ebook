# Performance Testing

What, exactly, is performance testing?  Before we can define "performance testing", we first need to define "performance".  I assume that at this point in the book, everybody understands what "testing" means.

Most technically savvy people will have a gut feeling for what performance means---systems that are "fast", that "don't take up much memory", etc.  The more you try to define it, though, the more slippery the definition becomes.  Faster, compared to what?  What if a system uses lots of RAM, and a competing system uses less RAM but more hard drive space?  If System A returns a response in three seconds, and System B returns a response in five seconds, which is more performant?  The answer may seem obvious, unless the queries to which the systems are responding are themselves different.  Performance is one of those concepts that is hard to pin down, because it will mean different things for different systems.

Imagine a video game where you play a software tester fighting evil Bugs.  You can use the arrow keys to move and the space key to shoot your insecticide gun.  Whenever you press the space key, you will expect your avatar on-screen to shoot the insecticide within, say, 200 milliseconds---anything more and you'll feel like the video game is "lagging" behind your keypresses.  Now imagine a second system, a weather forecasting supercomputing cluster, with the ability to run millions of calculations in parallel.  However, as weather forecasting requires a large number of calculations, it takes half an hour after hitting the "run" command for the system to return what the weather forecast is for tomorrow.  Until that time, the screen just says "Calculating..."  The supercomputer has several orders of magnitude longer response time than the video game, but does this mean the supercomputer is less performant than your video game console?  On the particular __performance indicator__ of response time, one could argue yes, but that performance indicator was not considered very important to the developers of the weather forecasting system.

How do you determine which system has better performance?  The short answer is, you can't!  The systems are trying to do different---and basically incomparable---actions.  How you measure performance will be based upon the kind of system you are testing.  That does not mean, however, that there are no rules or heuristics.  In this chapter, we'll go over some different kinds of performance and how one would go about testing them.  It is not an exhaustive list, though.  The system you are testing may have unique performance aspects which no book of finite length (which this book most assuredly is) could reasonably contain.

## Categories of Performance Indicators

As mentioned above, defining performance for a particular system means determining which performance indicators the user or customer cares about.  Although there are a large number of possible performance indicators, there are two main categories: service-oriented and efficiency-oriented.

A __service-oriented indicator__ measures how well a system is providing a service to a particular user.  These are measured from the user's perspective and quantify aspects of the system the user would directly care about.  Some examples of service-oriented indicators would be average page load time, response time, or what percentage of the time the system is available.

There are two main subcategories of service-oriented indicators. The first is availability---how available is the system to the user?  This could mean anything from what percentage of the time can the system be accessed to how available different features are to the users at different times.  The second main category is response time---how long does it take for the system to respond to the user's input or return a result?

An __efficiency-oriented indicator__ measures how efficiently a system makes use of available computational resources.  You can think of these as being measured "from the program's point of view", or at least from the computer's point of view.  Examples of efficiency-oriented indicators would be what percentage of CPU operations are being used for performing some functionality, how much disk space is being used, or how many concurrent users the system can handle on a particular server.  Although the last may seem like it may be a service-oriented indicator, the fact that it's from the system level---how many users can the system handle, rather than how does it appear to a specific user---means that it's an efficiency-oriented indicator.  Whenever you are measuring from the perspective of the system, as opposed to something _directly_ experienced by the user, you are testing an efficiency-oriented indicator.

Just like service-oriented indicators, there are two main subcategories in efficiency-oriented testing.  __Throughput__ measures how many events can the system handle in a given amount of time, such as how many users can log in at the same time, or how many packets can a system route in 30 seconds.  __Utilization__ measures what percentage or absolute amount of computing resources are used to perform a given task.  For example, how much disk space does a database take to store a specified table?  How much RAM is necessary when sorting it?

Service-oriented indicators are similar to black-box testing in that they measure the system from an external perspective, without determining the cause of the problem precisely.  As blunt instruments, they are broadly effective in finding where one should look more closely for problems.  Efficiency-oriented indicators are similar to white-box testing in that they require knowledge of the system as well as general technical knowledge.  Oftentimes, you can use service-oriented indicators to determine general areas where performance might be an issue to users, and then use efficiency-oriented indicators to nail down exactly what is causing the problem.  Long response times from the perspective of the user may result from physical RAM being used up and much of the information needing to be swapped in from disk.  Service-oriented testing would catch the former, but efficiency-oriented testing would be able to figure out the latter.

## Testing Performance: Thresholds and Targets

In order to determine whether or not a performance test has "passed", you need __performance targets__, or specific quantitative values that the performance indicators are supposed to reach.  For example, you may have an efficiency-oriented performance target that the installer for the software under test should be less than ten megabytes (it seems like a "Hello, world" program takes up over ten megabytes nowadays, but let's leave that complaint to the side for now).  You may have a service-oriented indicator that the system should respond within 500 milliseconds under normal load.  By quantifying a target, you can write a test and determine whether or not the system has met the target.

Targets, though, are the ideal.  Oftentimes, a particular performance indicator for a system may not reach its target, or may not be able to.  __Performance thresholds__ indicate the point where a performance indicator reaches an absolutely minimal acceptable performance.  For example, while the target for response time may be 500 milliseconds, systems engineers may have determined that the system would be releasable with 3000 millisecond response time.  It would not be great if it only met that mark, but if it takes any longer, the system would probably not be releasable.

A system whose performance indicators merely meet the threshold should not be considered as "passing" that particular performance metric.  There is still work to do!  You can see that the standard "pass"/"fail" metric often used for functional testing is often not really appropriate for performance metrics.  There are shades of gray, and they are often able to be manipulated.  What kind of hardware are you running the tests on?  What operating system?  What other processes are running?  Which web browser are you using to access the site?  By changing these, the same system may show very different results on a performance test.

## Key Performance Indicators

Although there are a very large number of potential performance indicators for a system, there will always be a subset of them that you are most interested in.  A video game may require very fast response time and high availability, but very little need for minimizing disk space, CPU, or RAM usage.  A long-term daemon process running in the background will have very different requirements---minimal RAM and CPU usage are very important, but response time is really not important at all.  Only rarely will a user or another process interact with it, and when it does, it will be via signals and not expecting a near real-time response.

By selecting the most important performance indicators---referred to as __key performance indicators__ or __KPIs__---you can focus on testing the aspects of performance most important to the users.  This will then allow developers to tune the system to meet the needs of the users, instead of wasting time on parts of the system which the users will not tend to care about or which already have a sufficient level of performance.  Depending on the level of specificity of the requirements, it may be possible to determine what the KPIs are while designing the system.  In many cases, you will have to determine them via user testing, researching similar applications in the domain, or using your testing knowledge to estimate them.

Selecting key performance indicators should be done before testing begins; you should not test a bunch of indicators, then turn around and note which ones were important.  You should determine which ones are most important ahead of time, when designing the test plan for the system.  If you are following a more lightweight development process, with less significant upfront design, you should at least determine whether or not a particular performance indicator is a KPI before running the appropriate test.  This forces you to keep in mind which aspects of the system's performance are important and which are not before coming up with results.  If you try to pick out the KPIs afterwards, you may be tempted (even if subconsciously) to simply select the indicators that met their targets.  Leave that kind of thinking to advertisers.

## Testing Service Oriented Indicators: Response Time

The easiest way to measure response time, of course, is to simply follow this algorithm:

1. Do something
2. Click "start" button on stopwatch
3. Wait for response
4. When response is seen, click "stop" button on stopwatch
5. Write down how long it took

While this may be the easiest way to measure response time, it is far from the best.  There are a plethora of problems with this approach.  First, it is impossible to measure sub-second times; human response time is simply too variable and not fast enough.  You can't measure anything internal to the system if your only interface to the system is what you see.  It's very time-consuming, thus making it difficult to collect large datasets.  It's subject to human error (ever spend an entire day timing things?  At some point, you're bound to forget to click the "start" button, or accidentally click the "reset" button before you've written down the time).  It's an absolutely fantastic way to destroy tester morale (ever spend an entire day timing things?  At some point, you're bound to remember to go look for another job).  Because of all these issues (and more), performance testing is often done with the aid of tools.

Although testing in general is becoming increasingly automated, performance testing in particular tends to depend heavily on automated tests.  There can be quite a bit of variation in performance indicators from run to run, due to other variables over which you may have little control.  Examples of these variables include other processes running on a server, how much physical RAM was already being used, garbage collection runs, and virtual machine startup times, among others.  Often the only way to get a truly valid result is running a performance test for numerous iterations and statistically analyzing it (obtaining the mean, median, and maximum response times, for example).  The only way to gain a reasonable number of samples in a reasonable amount of time is to automate the process.

### What is Time?

Although this may sound like a philosophical question, it actually has very direct ramifications when testing response times.  To a computer, there are actually several different ways of measuring time.  To a performance tester, how you report and measure time will depend upon what factors you are most interested in for the specific performance test.

The first kind of time, and probably the easiest to understand, is __real time__.  This is the kind of time measured by the clock on the wall, and is thus also referred to as __wall clock time__.  (And you thought technical terms would be difficult to learn.)  This is analogous to stopwatch time---how long does it take, from the user's perspective, for a program to do something?  Thus, this is usually the kind of time that users will care most about.  However, it does not tell the whole story.

Real time takes into account _everything_ that needs to be done by the system to perform the given task.  If this means reading data in from the disk, or over the network, that counts towards the real time total---no matter how slow the disk or bandwidth-restricted the network.  If this means that it's running on a system which is running lots of other processes concurrently, meaning that it only got a chance to use 5% of the CPU's time, then that counts towards the real time.  Real time is often not a great metric to track because it takes into account many things that are entirely out of the hands of the developers of the system.

__User time__, by contrast, measures specifically how much time was spent executing user code.  This does not count any time waiting for input, reading from the disk, time when another process has control of the processor, or even executing system calls.  It is very focused on that section of the system which developers have direct control over.  After all, you can easily change the sorting algorithm you use in your code, but if you need to get the resolution of the clock running on a system, the only way to do that is through a system call whose code you have no control over (unless you want to, for example, fork Linux).

The time spent in system calls---where code is being executed, but the code that is being executed is in the kernel and not in your program---is known as __system time__.  While developers do not have direct control over system time since they did not write the code, they do have indirect control since it is possible to reduce the number of system calls the program makes, change the ordering that they are made, call different functions, or use other techniques to minimize system time.

By adding together user time and system time, you get __total time__---the amount of time spent executing code either in user space or kernel space.  This is a good measurement of how much time your code is actually being executed.  It avoids calculating time that other processes were being executed on the processor or the system was waiting for input and allows focus simply on how long the code was being executed.  However, focusing on system time may blind you to issues related to these external factors.  For example, if you are testing database software, then much of your time will be spent reading from, and writing to, a disk.  It would be foolish to discount all of this time---even if the developers have no direct control over it, it should be taken into account.

Depending on what you're measuring and how much control you have over the test environment, tracking user, total, or real time might be the optimal indicator.  System time rarely is, unless you are testing a new kernel or operating system or deeply care about the split for arcane reasons.  Keep in mind that while developers have the most control over user time, indirect control over total time, and only minimal control over real time in most instances, users usually only care about real time!  If a user of your system finds it slow, they will not accept that you can't control how fast disk reads are as an excuse.  From a testing perspective, you should generally focus on measuring real time, avoiding as many extraneous factors, such as other programs running concurrently, as possible.  However, for certain processes which are CPU-bound or tend to operate in the background, focusing on total or user time may be more appropriate.

On most Unix-like systems (e.g. Linux or OS X), you can very easily get these values to benchmark a program.  Simply run `time <command>`, and after the command is finished executing, the real, user, and system times will be displayed (total time is easily calculated by adding user and system time):

```
$ time wc -w * | tail -n 1
   66156 total

real	0m0.028s
user	0m0.009s
sys	0m0.014s
$
```

There are various timing programs available for Windows machines, such as `timeit.exe` which comes with the Windows Server 2003 Resource Kit, but there is no almost universally-installed command such as `time` available.

### What Events Should Be Timed?

This will depend on what your performance indicator is.  If you have a specific indicator, either spelled out in the requirements or agreed to by the team, then measuring is a straightforward process.  However, if you need to determine this on your own, then your first step should be to figure out which events are important to the user of the system, where slow response times would cause hardship or annoyance.

If you are testing a system which is running on a server, some events you might want to think of checking for response time are:

1. Page load time (for web servers)
2. Download time
3. Connection response time
4. Time for data to appear on-screen on client
5. Time between connection and readiness for user input

For local programs, some events you may want to check for response time are:

1. Total execution time
2. Time from start of execution until ready for input
3. Time between input and response
4. Time between input and confirmation of input (e.g., a "loading" indicator)

Additionally, you may have some more specific response times to check for based on the program you are using.  For example, imagine a program which allows students to register for classes.  There may be one performance indicator for how long it takes to list all classes in a department and a separate performance indicator for how long it takes to enroll in a selected course.  Not all response times are created equal, and some may be key performance indicators while others are not.  For example, since students probably list courses much more often than enroll in them, listing all courses may be a KPI, whereas enrolling is measured but does not have any specific threshold or target.

Just because a particular indicator does not have a target or threshold does not mean it cannot be measured.  Oftentimes, especially if the performance indicator measurement is automated, it makes sense to just gather response times for many different events and store them.  These can be analyzed in an exploratory manner to see if there are any oddities or problems.  For example, even if enrollment time was not listed as a performance indicator, but we notice that it takes five minutes on average for a student to enroll in a class, it may be worthwhile to take a look for a chance to lower that time.  Not all performance requirements will be specified clearly before development, so it is often up to the tester to bring problems to the attention of the team even if the tester wasn't specifically looking for those issues.

Determining what kind of target time is acceptable for response time can be difficult.  However, there are some rough guidelines.  These were taken from _Usability Engineering_ by Jakob Neilsen.

* _< 100 ms:_ Response time required to feel that system is instantaneous
* _< 1 s:_ Response time required for flow of thought to not be interrupted
* _< 10 s:_ Response time required for user to stay focused on application (and not go see what's happening on the Internet)

Although these are rough guidelines for targets, they are not laws in and of themselves, and good response times will depend on a whole host of factors (what kind of network the system is running over, what kind of calculations the system is running, etc.)  There is empirical evidence, based on studies from Google, that people will choose websites, even if not consciously, based on less than 250 ms difference in load times.  Any load time over 400 milliseconds for a web page causes a drop-off in visitors, and this time has been decreasing steadily.  Users are becoming less patient as time goes on.

As a final note, it is possible for response time to be too fast!  Think of a scroll box that scrolls so fast that the user can't control it, or a screen which changes so quickly that it disorients the reader.  A performance indicator for response time might include an upper bound and a lower bound in some instances.

## Testing Service-Oriented Indicators: Availability

Availability, often referred to as uptime, refers to what percentage of the time that a user can access the system when they expect to be able to do so.  Thus, any time that a system spends being down reduces availability whether due to maintenance, an uncaught exception crashing a process, or a hard drive blowing up.

Many cloud service providers provide a __service level agreement__ (SLA) which specifies the level of availability that they provide.  This is often specified in the __n nines__ format, which specifies "how many nines" of reliability they provide.  These "nines" refer to 99% (2 nines), 99.9% (3 nines), 99.99% (4 nines), etc.  At the time of this writing, for example, Amazon S3 promises to have 3 nines' availability (available 99.9% of time, meaning downtime of less than around 45 minutes per month).  For certain systems, such as autonomous vehicles and space probes, uptime requirements might be much higher.

How do we determine what an appropriate level of availability is for a system under test?  If it is a key performance indicator, it will probably be specified as a requirement.  If not, however, it may be necessary to discuss with systems engineers to determine the proper level of availability for your system.  Note that increasing the level of availability one "level" of nines gets more and more difficult and will require larger amounts of engineering and design.  For example, going from 1 nine (90% availability, or less than approximately 40 days of downtime per year) to 2 nines (99%, or less than approximately four days of downtime per year) availability may involve simply adding some backup software and a daemon which can reboot processes when they fail.  Going one step further, to 99.9% availability (downtime of less than 9 hours per year) may require additional servers as backup and a redesign of the system to be more distributed.  Going further on, you may need to do formal verification of your software, buy hot-swappable hard drives, etc.  For example, if you are attempting to have 6 nines' reliability, that's approximately 30 seconds of downtime per year.  Everything that can go wrong---power outages, network disconnects, meteor strikes, etc.---will have to be dealt with automatically.  At those levels of availability, no human in the loop would be fast enough to prevent a level of downtime which would ruin your performance target.

This doesn't really answer the question, because there is no one answer.  The level of availability will depend on the domain of the software you're testing and the particular business or other needs of the user.  Even large web services have noticeable amounts of downtime during which you won't be able to check mail or see what inane, easily refuted stories your friends are posting on social media.  Some systems---such as avionics or spacecraft control software---are continuously available for years or even decades at a stretch, with no downtime.  Others, such as research software, may be down more than they are up.  As a tester, though, you can ask several question to determine what an acceptable level of availability would be:

1. __What is the downside if the system is not available?__ For research software, an experiment may be delayed.  For a website, it could mean that a business loses money.  For avionics software, it could mean a plane crash.
2. __How much effort is the team willing and able to put into availability?__ If the team has no internal impetus to work on increasing availability, and no money or resources are available to work on it, it may not be worthwhile to spend much time testing it.
3. __How does a focus on availability hamper other aspects of the software?__ Every minute you spend testing availability means less time testing other aspects of the system.  In some systems, your availability tests may help to uncover other defects.  In others, it may be a pure drain on other testing of the system.

By asking these questions, you can, in tandem with developers and other stakeholders of the system, determine what a good level of availability is.  Remember that target _and_ threshold levels can be set!  Most users would of course say that they would prefer that a system be available 100% of the time, and there's no issue with having that as a target.  However, by probing more deeply---e.g., would you be willing to pay three times as much for a system that's available 99.999% of the time, as opposed to 99.9% of the time?---you can often determine a good threshold value for this performance indicator.

Once we have determined what the target and threshold levels for availability are, how do we test them?  A simple answer might be to just run the system for a year, determine what amount of time the system was up and running, and divide by the total time to get the percentage of uptime per year.  Boom, we have our results!  This, of course, assumes that the business is perfectly fine waiting for a year to get results, and that no further development will take place during that time.  After all, if different code was deployed, that would ruin the experiment!  There are not many systems where this would be a reasonable way to determine availability.  Just like we determined with using a stopwatch for response time testing, the simple solution is not feasible to use in practice.

Instead, one can model the system to be tested.  A simple model may involve running the system for one day, noting how many failures occur and calculating how much time it would take to fix them.  Divide this time by the time in a day, and you theoretically have how much availability the system will have over any length of time.  If the system experiences two problems eleven hours and fifty minutes apart, for instance, each of which caused it to not be available for ten minutes, then you can add up 2 * 10 = 20 minutes, divide it by 24 hours (1440 minutes), and you had an availability that day of 20 / 1440, or 98.6% uptime---not quite two nines.

In fact, this is the general idea behind the standard model of availability.  Once you calculate the mean time between failures (MTBF) and the mean time to repair (MTTR) for those failures, you can just use the following equation as a rough estimate of system availability:

```
(MTBF / (MTBF + MTTR)) * 100%
```

In our example above, the mean time between failures is 710 minutes, and the mean time to repair is 10 minutes.  Thus, MTBF + MTTR is 720 minutes.

```
(710 / (710 + 10)) * 100% = 98.61% availability
```

This simple model fails to take into account that components tend not to fail at the same rate throughout time.  Although you will be able to see some of the random failures that a system will inevitably hit, and probably more than a fair share of problems resulting from the first time that code is executed in a real environment, this error rate will probably not reflect a longer-term rate of error in the system.  Along with the "ramp-up failures" and "random failures", you also have to take into account that hardware will wear out. A second factor is that things which you know are likely to happen, but did not during the day of test, will certainly occur if the system is running for any reasonable length of time---for example, power outages or cooling system failures.  Just because you didn't see them that day does not mean that it will never happen, just as if you rolled a die and got a 4 means that you can assume that you will never get a 6.  Finally, there may be what Donald Rumsfeld famously called "unknown unknowns"---situations that you and the system designers never took into consideration or even imagined happening.  According to astronomer Phil Plait, your chances of dying via meteor strike are approximately 1 in 700,000---slightly less than your chances of dying in a firework accident and slightly more than your chances of dying in a terrorist attack.  Can you assume that the data center where your site is hosted will survive indefinitely and not be struck by a meteor?  The longer the system is running, the higher the chances that these small-probability events will occur, and these will be difficult to catch using the naÃ¯ve model of simply multiplying one day's worth of results by the number of days in a year.

Systems tend to follow what is called the __bathtub curve__.  This means that many failures tend to happen at the beginning, when the system is just starting up, then the number of failures decreases, then finally starts to increase again as components wear out, libraries stop being updated, external APIs stop being supported, etc.  When plotted out, the curve looks like a side view of a bathtub.  There's a high amount of downtime and errors at first, then a drastic drop-off in downtime and error rate as the obvious problems manifest themselves and are fixed, then a long period of relatively low amount of downtime, then a slope upwards as the system gets older and things start to go wrong.  Including the bathtub curve in your model planning can be helpful, although it's only a rough guide.  You should assume that the system will have more downtime at the beginning as your team sees and fixes defects that were only observable once the system started running "for real".

For your particular system, you may want to determine how much more complicated you want to make the model, how much time you want to spend gathering data, and how many external factors such as outside APIs being down or other problems you would like to take into consideration.  Remember that in performance testing in general, it is rarely a good idea to use one data point to generalize.  If you are going to use a "day" as the basis for your calculations, don't use just one day's worth of testing to determine the mean time to failure and mean time to repair.  Remember to factor in at least some room for things that you didn't think of that will go wrong---remember Murphy's Law.  All that being said, the basic concept of (MTBF / (MTBF + MTTR)) should provide a nice starting point for any given system to determine its level of availability.

Now that you know what you're looking for and how you're going to deal with the results, it's time to generate those numbers.  This can be done by __load testing__.  In load testing, a system is set up and monitored while events are processed.  The kind of events will depend on the system under test---for a web server, for example, it might be page loads, or for a router, it would be packets routed.  By using different kinds of load tests based on realistic assumptions, one can determine the MTBF and MTTR, and thus the availability that users can expect to see.

A __baseline test__ runs the system with few or no events to process, just to check that the system can in fact handle running for a given period of time.  This is often not a realistic scenario---a system which never does much is not likely to be developed in the first place---but provides a "baseline" which future tests can compare results against.  The opposite of a baseline test is a __stress test__, in which the system is "stressed" by having it process a large number of events in small time frame.  Stress tests are often not run for a long period of time, but useful for modeling those periods when the system is under heavy stress---for example, a DDoS attack for a website, or the beginning of the Fall semester for a class registration system.

A __stability test__, sometimes also called a __soak test__, is often the single most realistic load test that is run.  During this kind of load test, a constant but small-to-medium number of events are processed over a long period of time, in order to determine that the system is in fact stable when doing realistic work.

If at all possible, determine what real-world usage is like, so that you can model your system appropriately.  It's not at all uncommon to combine various load tests together to come up with single MTBF and MTTR values.  For example, you may be able to determine that the system under test spends 10% of its time in a "stress state" and 90% of its time in a "stability state".  If the MTBF is 10 minutes during a stress state, and 12 hours during a stability state, with a MTTR of 5 minutes in each case, you can create a more nuanced calculation:

```
MTBF(stress) = 10
MTBF(stable) = 720
MTTR = 5
MTBF = (0.1 * MTBF(stress)) + (0.9 * MTBF(stable)) = 649
(MTBF / (MTBF + MTTR)) * 100
(649 / (649 + 5)) * 100 = 99.23% availability
```

This is just an example, and it could certainly be further enhanced (and any state of the system that reaches such high levels of unavailability during stress states probably should be looked at further) .  Some systems do have spikes of usage, where others do not.  Some systems will handle lots of small events, others will handle a smaller number of harder-to-process events.  If you are able to gather real-world data and metrics about the usage of your system, you will be able to model its behavior better, and thus produce better availability numbers.  The more realistic the data, the more realistic the model you will be able to create.  Even with the most realistic of data, however, be prepared to be wrong---there are so many things that can go wrong with a complex system, it is impossible to take all of them into account in any model.

## Testing Efficiency-Oriented Indicators: Throughput

If you'll recall from earlier, efficiency-oriented indicators take a view of the system from the perspective of the system and how efficiently it makes use of the computation resources available to it.  One measure of efficiency is the amount of __throughput__, or number of events that can be processed in a given amount of time on a specified hardware setup.  Examples would include how many web pages a web server could serve in one minute, or how many SQL queries a database server could perform in one minute.   This may seem similar to response time, or at least its inverse, but it isn't quite so simple.  While response time was measuring the response time from the point of view of a particular user of the system, throughput is how performant the system as a whole is in responding to a number of users.  If I am simply using a web server, I don't care about how fast other users get their data.  If I am an administrator of a web site using a particular server, then I certainly do care!

Just as we did when testing availability, we can use load testing to determine the throughput of the system.  Instead of determining if the system is up however, we can specifically check for what sustained rate of events can occur before some predetermined lower threshold on performance is reached.  For example, in our web server example, how many pages per minute can the web server serve before the average response time dips below 3 seconds?  For a web-based course registration system, how many students can view courses at the same time before the database request queue is saturated?  The exact parameters of what counts as "below the performance threshold" will vary by system, but it is commonly average or maximum response time.  We are using a service-oriented indicator as part of testing an efficiency-oriented indicator!  This goes to show that different aspects of performance are often intertwined; poor throughput could be the cause of slow response time, or poor utilization of resources could directly lead to a lack of availability.

Determining the values for the lowest threshold will be heavily dependent on the KPIs for this particular system.  Once again, these should be determined before testing starts, so as not to cherry-pick the results.  Once they are determined, you should ramp up the number of events until the system can no longer perform within the determined performance threshold.

Remember to track the level of events and the equivalent throughput, and if possible, relevant utilization measures (see next section for more on measuring utilization).  This will allow you to see if there are any patterns in the throughput levels, thus allowing you to extrapolate further than the data points you have collected.  For example, assume that you see that response time is always approximately 500 millisecond up to 100 events per second, but at 150 events per second, response time slows to 800 milliseconds and at 200 events per second, it is measured to be 2500 milliseconds.  We can see some superlinear growth starting at approximately the 100 events per second mark, and developers will then have more information to start tracking down bottlenecks.  Is that the point where data starts being swapped to disk?  Do we have 100 threads, and this is the point where the thread pool runs dry?  Although you may not have these answers up front, having an idea of the growth, and especially a general area where throughput starts to slow down, will make improving the performance much easier.  Just as we discussed regarding filing defects, being more specific is almost always better than being less specific!

Throughput levels are very sensitive to what kind of hardware you are running the software on, so even more than with other tests, you will need to ensure that you are running on the same kind of hardware from run to run to ensure valid results.  You may also want to determine throughput using the same software on different hardware configurations, which may also help to track down the cause of any slowdowns.  For example, a system which has extremely reduced throughput when run on a system with slightly less RAM than another may indicate a memory bottleneck.

## Testing Efficiency-Oriented Indicators: Utilization

Testing __utilization__ means determining what amount of computing resources---on an absolute or relative basis---a particular system when performing some functionality.  Computing resources is a very broad term, covering anything that a program could be "using" on a machine. Common examples include:

1. CPU
3. Disk usage
4. RAM
5. Network usage

While these are some of the most common resources measured, there are also very specific ones which can be measured.  The number of disk bytes read?  Standby cache normal priority bytes?  Number of C3 transitions on your CPU in the last second?  All of these (along with literally thousands of others) can be tested right out of the box on your Windows machine by using a tool called `perfmon`.  For other operating systems, there are equivalent tools, such as Activity Monitor for OS X or `sar` for Linux.  However, these very specific measurements are usually only needed after discovering a problem with the more generalized measurements enumerated above.

Often, you only need a "finger in the wind" estimate of resource usage, or to see which process is using the most resources.  This can be done by using tools that are freely available on various operating systems: Windows systems have Task Manager, while OS X and Linux systems use top.  Each of these programs will enable you to see which programs you have running that are using the most CPU or memory.  I'll skip over the particulars of running them; you can use `man top` to get the Unix manual entry for top, or look up the appropriate Microsoft documentation for Task Manager.

Both of these tools are similar in showing snapshots of resource usage at a particular moment in time.  They'll let you see if there is a particular spike in CPU usage when you perform some action, or how the memory usage of a process will vary as time goes on.  For a Java program, for example, you will often see the telltale sign of a garbage-collected program, as memory usage drifts up, then suddenly comes back down, in a "sawtooth" pattern.

While service-oriented indicators can often give a warning that something is wrong with the system, using a tool such as top will allow you to drill down just a bit further, determining, say, that slow response times (service-oriented indicator) seems to be the result of the CPU going to 99% usage every time a result is asked for.  However, looking at resource utilization this way is a very blunt instrument.  Modern CPUs are complex, modern software is complex, modern memory is complex, and together, they mean that trying to understand the resource utilization of a system is complex.  Instructions can be pipelined or threads can be running on different cores, there are different levels of cache and different kinds of memory usage, software may operate differently and use resources in different ways on different systems.  More importantly, once you've determined that the problem is extreme CPU usage or too much memory being used, how do you put that in terms of a defect that a developer can address?

The answer is that you get more specific via a __profiler__ (also known as a __profiling tool__).  A profiler, such as JProfiler or VisualVM for Java programs, will allow you to not only see that the process is using lots of CPU, but how much CPU each particular method is using.  It's much easier for a developer to track down an issue when they know in what method the problem is likely to be!  Instead of simply letting you know that a system is taking up 100 megabytes of memory, you can see which objects have been instantiated, and which classes they are instantiations of.  While this will help you narrow in on the problem, trying to determine whether or not there _is_ a problem in the first place by using a profiler might be overkill.  It is very easy to become overwhelmed with data, if you do not have a specific target.

If you determine that the problem is with network usage, you can use a __packet analyzer__ such as Wireshark to inspect individual packets or run statistical analyses.  This will allow you to see specifically which packets are being sent or received and may be causing a bottleneck, or if there is superfluous data being sent out which may be a cause of performance degradation.  For example, while timestamps may be necessary to send out on a regular basis, is the inclusion of the timezone of the originating system really necessary?  Could timestamps simply always be in UTC and adjusted locally according to the settings of each receiving system?

## General Tips and Advice When Performance Testing

1. Use a tool for any non-trivial performance measurement.  Relying on humans to execute many performance tests will lead to mistakes due to human error, as well as increasing the possibility of demoralized testers.
2. Compare apples to apples; keep variables the same between runs of the same test!  Don't run the first version of the software on a laptop and the second version on a supercomputer and proclaim a massive increase in speed.
3. Be mindful of startup and/or shutdown costs.  If a system needs to start up a VM (e.g., a JVM for all Java processes), then you may need a way to standardize that across all runs, ignore the first result since it will include startup times, or otherwise take it into consideration.
4. Be mindful of caching.  If a system is caching results, you may see very different performance measurements the first time running a test versus the second and third times.
5. Have control over the testing system.  You want to make sure that others are not logged in to the system, that settings are the same between runs, that no additional software has been installed, etc.  Remember that performance testing is like running a science experiment---if you can't reduce extraneous variables, you won't be able to trust it.
6. Have good input data.  If you can, try to test with real, or at least possibly real, data.  Many times the kind of performance that you see on a system will depend upon data.  If you used a bubble sort, but all of your test data was in order, you may not notice the bad performance that comes with normally unordered data, since bubble sort is O(n) for already-sorted data but O(n^2) in the average case.
7. Don't trust a single test run.  Although you endeavor to remove all extraneous variables from your performance test, there will always be elements you can't control, from how the memory is allocated to which order threads are run.  While these may or may not have a significant impact on the performance of your system, they are always there.  The only way to minimize their impact is to run the test multiple times and hopefully smooth out the data by taking the mean result or otherwise analyzing it statistically.
8. Keep track of your test runs, and store the data.  This will allow you to determine when problems first started appearing, problems which you may not have noticed at first glance.  It will also let you determine trends.  For example, you may notice that with each successive version of the software you test, memory usage is going up while CPU usage is going down.
9. Finally, consider how much performance testing is necessary and that you have to do.  If your system only executes overnight, does response time actually matter that much?  If the system does take up too much CPU, is that something that will be prioritized to fix?  Just like with any other kind of testing, time spent looking for problems in one place inevitably means less time to look for problems of other kinds.  Be sure that you are using your time, limited as it is, wisely.
